{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5865970,"sourceType":"datasetVersion","datasetId":3372843},{"sourceId":7514565,"sourceType":"datasetVersion","datasetId":4377034},{"sourceId":7533033,"sourceType":"datasetVersion","datasetId":4387422}],"dockerImageVersionId":30498,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment was run through kaggle notebooks\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# The image is functionally a jupyter notebook, although items were saved to subfolders on the kaggle cloud storage system\n\n# In order to run this code on another system, please make sure that Python 3 is installed correctly, and change any path names to match\n# your computer system\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nfrom collections import defaultdict\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nsubfolder_path = 'kaggle/output/outputdata'\n\nif not os.path.exists(subfolder_path):\n    os.makedirs(subfolder_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-01T18:27:23.006313Z","iopub.execute_input":"2024-02-01T18:27:23.007132Z","iopub.status.idle":"2024-02-01T18:27:23.035094Z","shell.execute_reply.started":"2024-02-01T18:27:23.007099Z","shell.execute_reply":"2024-02-01T18:27:23.034123Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/amar-protest/protes.csv\n/kaggle/input/short-articles/training_data_for_short_article_testing.csv\n/kaggle/input/short-articles/100_short_articles_testing.csv\n/kaggle/input/indices-amar/test_indices_fold_0.csv\n/kaggle/input/indices-amar/train_indices_fold_1.csv\n/kaggle/input/indices-amar/downsampled_index.csv\n/kaggle/input/indices-amar/train_indices_fold_0.csv\n/kaggle/input/indices-amar/test_indices_fold_1.csv\n/kaggle/input/indices-amar/train_indices_fold_4.csv\n/kaggle/input/indices-amar/test_indices_fold_2.csv\n/kaggle/input/indices-amar/train_indices_fold_2.csv\n/kaggle/input/indices-amar/indices.txt\n/kaggle/input/indices-amar/data_downsampled.csv\n/kaggle/input/indices-amar/train_indices_fold_3.csv\n/kaggle/input/indices-amar/test_indices_fold_4.csv\n/kaggle/input/indices-amar/test_indices_fold_3.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip uninstall -y transformers\n!pip install --upgrade accelerate\n!pip install transformers==4.28.0","metadata":{"execution":{"iopub.status.busy":"2024-02-01T18:27:23.036883Z","iopub.execute_input":"2024-02-01T18:27:23.037190Z","iopub.status.idle":"2024-02-01T18:27:56.857260Z","shell.execute_reply.started":"2024-02-01T18:27:23.037164Z","shell.execute_reply":"2024-02-01T18:27:56.856132Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nCollecting accelerate\n  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.28.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.64.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.26.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting transformers==4.28.0\n  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (2023.5.7)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.29.2\n    Uninstalling transformers-4.29.2:\n      Successfully uninstalled transformers-4.29.2\nSuccessfully installed transformers-4.28.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# For machine learning tools and evaluation\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\n\n# For deep learning\n# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\nimport torch\n\n# For plotting and data visualization\n#import matplotlib\n\n#import matplotlib.pyplot as plt\n#import seaborn as sns\n#from matplotlib import ticker\n\n#sns.set(style='ticks', font_scale=1.2)\n\n# transformers\nfrom transformers import AutoTokenizer, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\n\nmodel_name = 'bert-base-cased'\n\ndevice_name = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-02-01T18:27:56.859082Z","iopub.execute_input":"2024-02-01T18:27:56.859382Z","iopub.status.idle":"2024-02-01T18:28:09.541247Z","shell.execute_reply.started":"2024-02-01T18:27:56.859354Z","shell.execute_reply":"2024-02-01T18:28:09.540426Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# This code loads the news articles, removes short articles, cleans the data frame to fit the transormers format and saves the data to csv\ndf = pd.read_csv(\"/kaggle/input/amar-protest/protes.csv\")\ndf['Protest']\ndf['labels'] = df['Protest'].apply(lambda x: 1 if x == 'Protest' else 0)\ndf = df.rename(columns={'body': 'text'})\ndf = df[['labels', 'text']]\ndf = df[df['text'].notna() & (df['text'] != '')]\ndf = df[df['text'].str.len() >= 50]\n# Separate the majority and minority classes\ndf_majority = df[df['labels'] == 0]\ndf_minority = df[df['labels'] == 1]\n\n# Down-sample the majority class\ndf_majority_downsampled = df_majority.sample(n=len(df_minority), random_state=0)\ndownsampled_index = df_majority_downsampled.index\n# Combine the down-sampled majority class with the minority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n\n# Save files\ndf_downsampled.to_csv('/output/outputdata/data_downsampled.csv', index=False)\nindex_series = pd.Series(downsampled_index)\nindex_series.to_csv('/output/outputdata/downsampled_index.csv', index=False, header=['index'])","metadata":{"execution":{"iopub.status.busy":"2024-01-30T17:31:39.210358Z","iopub.execute_input":"2024-01-30T17:31:39.212042Z","iopub.status.idle":"2024-01-30T17:31:39.599441Z","shell.execute_reply.started":"2024-01-30T17:31:39.212013Z","shell.execute_reply":"2024-01-30T17:31:39.598578Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"seed = 41\nn = 5\nskf = StratifiedKFold(n_splits=n)\n\nskf.get_n_splits(df_downsampled['text'], df_downsampled['labels'])\n\n# Save the indices to a CSV file\nfor i, (train_index, test_index) in enumerate(skf.split(df_downsampled['text'], df_downsampled['labels'])):\n    np.savetxt(f'/output/outputdata/train_indices_fold_{i}.csv', train_index, delimiter=',')\n    np.savetxt(f'/output/outputdata/test_indices_fold_{i}.csv', test_index, delimiter=',')\ntokenizer = AutoTokenizer.from_pretrained(model_name)  # The model_name needs to match our pre-trained model.\n\nX = df['text'].values\nY = df['labels'].values\n\n# Convert the labels to a numpy array\nY = np.array(Y)\n\n# Initialize results storage\nresults = []","metadata":{"execution":{"iopub.status.busy":"2024-01-30T17:33:37.288721Z","iopub.execute_input":"2024-01-30T17:33:37.289084Z","iopub.status.idle":"2024-01-30T17:45:20.920432Z","shell.execute_reply.started":"2024-01-30T17:33:37.289056Z","shell.execute_reply":"2024-01-30T17:45:20.919246Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class MyDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-02-01T18:29:08.474306Z","iopub.execute_input":"2024-02-01T18:29:08.475528Z","iopub.status.idle":"2024-02-01T18:29:08.482876Z","shell.execute_reply.started":"2024-02-01T18:29:08.475495Z","shell.execute_reply":"2024-02-01T18:29:08.481787Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"results = []\nconfusion_matrices = []  # Store confusion matrices for each fold\nclassification_reports = [] #store classification reports\nn_splits = 5  # Assuming you have 5 folds\n\n# This fold runs the BERT model across each fold\nfor i in range(n_splits):\n    # Define the file paths for train and test indices\n    train_indices_file = f'/kaggle/input/indices-amar/train_indices_fold_{i}.csv'\n    test_indices_file = f'/kaggle/input/indices-amar/test_indices_fold_{i}.csv'\n    # Load train and test indices from CSV files\n    train_index = np.loadtxt(train_indices_file, delimiter=',', dtype=int)\n    test_index = np.loadtxt(test_indices_file, delimiter=',', dtype=int)\n\n    # Subset your DataFrame to create train and test sets\n    X_train, X_test = df_downsampled['text'].iloc[train_index], df_downsampled['text'].iloc[test_index]\n    Y_train, Y_test = df_downsampled['labels'].iloc[train_index], df_downsampled['labels'].iloc[test_index]\n\n    # Convert train and test sets to lists for tokenization\n    X_train = X_train.tolist()\n    X_test = X_test.tolist()\n\n    # Tokenize the text data\n    train_encodings = tokenizer(X_train, truncation=True, padding=True)\n    test_encodings = tokenizer(X_test, truncation=True, padding=True)\n\n    # Create torch datasets using the custom dataset class MyDataset\n    train_dataset = MyDataset(train_encodings, Y_train.to_numpy())\n    test_dataset = MyDataset(test_encodings, Y_test.to_numpy())\n\n\n    # Load pre-trained BERT model and send it to the GPU\n    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device_name)\n\n    # Define training arguments\n    training_args = TrainingArguments(\n        num_train_epochs=3,  # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=20,  # batch size for evaluation\n        learning_rate=5e-5,  # initial learning rate for Adam optimizer\n        warmup_steps=100,  # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n        weight_decay=0.01,  # strength of weight decay\n        output_dir=f'./results/fold_{i}',  # Change output directory for each fold\n        logging_dir=f'./logs/fold_{i}',  # Change logging directory for each fold\n        logging_steps=100,  # number of steps to output logging (set lower because of small dataset size)\n        evaluation_strategy='steps',  # evaluate during fine-tuning so that we can see progress\n    )\n\n    # Initialize the Trainer\n    trainer = Trainer(\n    model=model,  # the instantiated Transformers model to be trained\n    args=training_args,  # training arguments, defined above\n    train_dataset=train_dataset,  # training dataset\n    eval_dataset=test_dataset,  # evaluation dataset (usually a validation set; here we just send our test set)\n    compute_metrics=compute_metrics  # our custom evaluation function\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    fold_results = trainer.evaluate()\n    \n    # Save the results for this fold\n    results.append(fold_results)\n\n    # Clean up CUDA memory\n    torch.cuda.empty_cache()\n    \n    # Predict on the test set\n    test_predictions = trainer.predict(test_dataset)\n    preds = np.argmax(test_predictions.predictions, axis=-1)\n\n    # Save predictions to a CSV file\n    preds_df = pd.DataFrame(preds, columns=['predictions'])\n    preds_df.to_csv(f'predictions_fold_{i+1}.csv', index=False)\n    \n    # Compute and store the confusion matrix\n    cm = confusion_matrix(Y_test, preds)\n    confusion_matrices.append(cm)\n    \n    # Compute and store the classification report\n    report = classification_report(Y_test, preds, output_dict=True)\n    classification_reports.append(report)\n\n    print(f'Fold {i} Classification Report:')\n    print(f'Precision: {report[\"weighted avg\"][\"precision\"]}')\n    print(f'Recall: {report[\"weighted avg\"][\"recall\"]}')\n    print(f'F1-score: {report[\"weighted avg\"][\"f1-score\"]}')\n\n    # Evaluate the model and append results\n    fold_results = trainer.evaluate()\n    results.append(fold_results)\n\n    # Clean up CUDA memory\n    torch.cuda.empty_cache()\n\n# After all folds, you can process and display the stored confusion matrices\nfor fold, cm in enumerate(confusion_matrices):\n    print(f'Fold {fold} Confusion Matrix:')\n    print(cm)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T17:33:37.288721Z","iopub.execute_input":"2024-01-30T17:33:37.289084Z","iopub.status.idle":"2024-01-30T17:45:20.920432Z","shell.execute_reply.started":"2024-01-30T17:33:37.289056Z","shell.execute_reply":"2024-01-30T17:45:20.919246Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/1899310376.py:52: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  train_index = np.loadtxt(train_indices_file, delimiter=',', dtype=int)\n/tmp/ipykernel_32/1899310376.py:53: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  test_index = np.loadtxt(test_indices_file, delimiter=',', dtype=int)\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240130_173354-39s2n3b7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/amar2/huggingface/runs/39s2n3b7' target=\"_blank\">happy-silence-5</a></strong> to <a href='https://wandb.ai/amar2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/amar2/huggingface' target=\"_blank\">https://wandb.ai/amar2/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/amar2/huggingface/runs/39s2n3b7' target=\"_blank\">https://wandb.ai/amar2/huggingface/runs/39s2n3b7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [138/138 01:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.617900</td>\n      <td>0.359679</td>\n      <td>0.846154</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 0 Classification Report:\nPrecision: 0.8561868686868688\nRecall: 0.8406593406593407\nF1-score: 0.8389036413026891\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_32/1899310376.py:52: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  train_index = np.loadtxt(train_indices_file, delimiter=',', dtype=int)\n/tmp/ipykernel_32/1899310376.py:53: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  test_index = np.loadtxt(test_indices_file, delimiter=',', dtype=int)\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [138/138 01:57, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.624300</td>\n      <td>0.490899</td>\n      <td>0.730769</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 1 Classification Report:\nPrecision: 0.8716853085333668\nRecall: 0.8571428571428571\nF1-score: 0.8557317073170732\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_32/1899310376.py:52: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  train_index = np.loadtxt(train_indices_file, delimiter=',', dtype=int)\n/tmp/ipykernel_32/1899310376.py:53: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  test_index = np.loadtxt(test_indices_file, delimiter=',', dtype=int)\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [138/138 01:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.586700</td>\n      <td>0.475064</td>\n      <td>0.774725</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 2 Classification Report:\nPrecision: 0.7817337461300309\nRecall: 0.7637362637362637\nF1-score: 0.7599018254333488\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_32/1899310376.py:52: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  train_index = np.loadtxt(train_indices_file, delimiter=',', dtype=int)\n/tmp/ipykernel_32/1899310376.py:53: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  test_index = np.loadtxt(test_indices_file, delimiter=',', dtype=int)\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [138/138 01:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.608800</td>\n      <td>0.583337</td>\n      <td>0.734807</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 3 Classification Report:\nPrecision: 0.7674289768438849\nRecall: 0.7403314917127072\nF1-score: 0.7332409546231636\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_32/1899310376.py:52: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  train_index = np.loadtxt(train_indices_file, delimiter=',', dtype=int)\n/tmp/ipykernel_32/1899310376.py:53: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n    * make sure the original data is stored as integers.\n    * use the `converters=` keyword argument.  If you only use\n      NumPy 1.23 or later, `converters=float` will normally work.\n    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n      floating point and then convert it.  (On all NumPy versions.)\n  (Deprecated NumPy 1.23)\n  test_index = np.loadtxt(test_indices_file, delimiter=',', dtype=int)\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [138/138 01:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.555200</td>\n      <td>1.051015</td>\n      <td>0.513812</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 4 Classification Report:\nPrecision: 0.7015139556576021\nRecall: 0.6574585635359116\nF1-score: 0.6384799163804689\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Fold 0 Confusion Matrix:\n[[67 24]\n [ 5 86]]\nFold 1 Confusion Matrix:\n[[69 22]\n [ 4 87]]\nFold 2 Confusion Matrix:\n[[81 10]\n [33 58]]\nFold 3 Confusion Matrix:\n[[82  9]\n [38 52]]\nFold 4 Confusion Matrix:\n[[80 10]\n [52 39]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# save results\n!zip -r file.zip outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This code trains a model just on the clean text used when prompt engineering GPT-4\n\ndf_test = pd.read_csv(\"/kaggle/input/short-articles/100_short_articles_testing.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/short-articles/training_data_for_short_article_testing.csv\")\ndf_train['text'] = df_train['body']\ndf_test['text'] = df_test['body']","metadata":{"execution":{"iopub.status.busy":"2024-02-01T18:32:16.647894Z","iopub.execute_input":"2024-02-01T18:32:16.648278Z","iopub.status.idle":"2024-02-01T18:32:16.702927Z","shell.execute_reply.started":"2024-02-01T18:32:16.648247Z","shell.execute_reply":"2024-02-01T18:32:16.702015Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'bert-base-cased'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)  # The model_name needs to match our pre-trained model.\n\n# Subset your DataFrame to create train and test sets\nX_train, X_test = df_train['text'], df_test['text']\nY_train, Y_test = df_train['labels'], df_test['labels']\n\n# Convert train and test sets to lists for tokenization\nX_train = X_train.tolist()\nX_test = X_test.tolist()\n\n# Tokenize the text data\ntrain_encodings = tokenizer(X_train, truncation=True, padding=True)\ntest_encodings = tokenizer(X_test, truncation=True, padding=True)\n\n# Create torch datasets using the custom dataset class MyDataset\ntrain_dataset = MyDataset(train_encodings, Y_train.to_numpy())\ntest_dataset = MyDataset(test_encodings, Y_test.to_numpy())\n\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device_name)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    num_train_epochs=3,  # total number of training epoch\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=20,  # batch size for evaluatio\n    learning_rate=5e-5,  # initial learning rate for Adam optimizer\n    warmup_steps=100,  # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n    weight_decay=0.01,  # strength of weight decay\n    output_dir=f'./results/short_articles',  # Change output directory for each fold\n    logging_dir=f'./logs/short_articles',  # Change logging directory for each fold\n    logging_steps=100,  # number of steps to output logging (set lower because of small dataset size)\n    evaluation_strategy='steps',  # evaluate during fine-tuning so that we can see progress\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,  # the instantiated Transformers model to be trained\n    args=training_args,  # training arguments, defined above\n    train_dataset=train_dataset,  # training dataset\n    eval_dataset=test_dataset,  # evaluation dataset (usually a validation set; here we just send our test set)\n    compute_metrics=compute_metrics  # our custom evaluation function\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\nresults = trainer.evaluate()\n    \n# Clean up CUDA memory\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T18:36:52.393910Z","iopub.execute_input":"2024-02-01T18:36:52.394783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test set\ntest_predictions = trainer.predict(test_dataset)\npreds = np.argmax(test_predictions.predictions, axis=-1)\n\n# Save predictions to a CSV file\npreds_df = pd.DataFrame(preds, columns=['predictions'])\npreds_df.to_csv(f'predictions_short_articles.csv', index=False)\n# Compute and store the confusion matrix\ncm = confusion_matrix(Y_test, preds)\n    \n# Compute and store the classification report\nreport = classification_report(Y_test, preds, output_dict=True)\n\nprint(f'Short article Classification Report:')\nprint(f'Precision: {report[\"weighted avg\"][\"precision\"]}')\nprint(f'Recall: {report[\"weighted avg\"][\"recall\"]}')\nprint(f'F1-score: {report[\"weighted avg\"][\"f1-score\"]}')\n\ncm","metadata":{"execution":{"iopub.status.busy":"2024-02-01T18:52:45.499064Z","iopub.execute_input":"2024-02-01T18:52:45.499695Z","iopub.status.idle":"2024-02-01T18:52:47.316919Z","shell.execute_reply.started":"2024-02-01T18:52:45.499663Z","shell.execute_reply":"2024-02-01T18:52:47.315984Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Short article Classification Report:\nPrecision: 0.920673076923077\nRecall: 0.92\nF1-score: 0.919967987194878\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([[47,  3],\n       [ 5, 45]])"},"metadata":{}}]}]}